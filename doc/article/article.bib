@inproceedings{rcnn,
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
	volume = {28},
	year = {2015}
}

@article{arrowrcnn,
	author    = {Benedikt Schäfer and Margret Keuper and Heiner Stuckenschmidt},
	title     = {Arrow R-CNN for handwritten diagram recognition},
	journal   = {International Journal on Document Analysis and Recognition (IJDAR)},
	volume    = {24},
	pages     = {3--17},
	year      = {2021},
	doi       = {10.1007/s10032-020-00361-1},
	url       = {https://doi.org/10.1007/s10032-020-00361-1}
}

@inproceedings{interactiveUML,
	author = {Lank, Edward and Thorley, Jeb and Chen, Sean},
	year = {2000},
	month = {01},
	pages = {7},
	title = {An interactive system for recognizing hand drawn UML diagrams.},
	doi = {10.1145/782034.782041},
	url = {https://dl.acm.org/doi/10.5555/782034.782041}
}

@inproceedings{sketchdiagram,
	title={Sketch2Diagram: Generating Vector Diagrams from Hand-Drawn Sketches},
	author={Itsumi Saito and Haruto Yoshida and Keisuke Sakaguchi},
	booktitle={The Thirteenth International Conference on Learning Representations},
	year={2025},
	url={https://openreview.net/forum?id=KvaDHPhhir}
}

@inproceedings{microsofttrocr,
	author    = {Li, Minghao and Lv, Tengchao and Chen, Junlong and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
	title     = {TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	year      = {2023},
	volume    = {37},
	number    = {11},
	pages     = {13094--13102},
	doi       = {10.1609/aaai.v37i11.26538},
	url 	  = {https://doi.org/10.1609/aaai.v37i11.26538},
	publisher = {AAAI Press}
}

@INPROCEEDINGS{dbscan,
	author={Deng, Dingsheng},
	booktitle={2020 7th International Forum on Electrical Engineering and Automation (IFEEA)}, 
	title={DBSCAN Clustering Algorithm Based on Density}, 
	year={2020},
	volume={},
	number={},
	pages={949-953},
	keywords={Machine learning algorithms;Clustering algorithms;Machine learning;Big Data;Prediction algorithms;Data mining;Unsupervised learning;DBSCAN Algorithm;Density Clustering;Machine Learning;Algorithm Research},
	doi={10.1109/IFEEA51475.2020.00199},
	url={https://doi.org/10.1109/IFEEA51475.2020.00199}
}

@article{SIFT,
	author    = {David G. Lowe},
	title     = {Distinctive Image Features from Scale-Invariant Keypoints},
	journal   = {International Journal of Computer Vision},
	year      = {2004},
	volume    = {60},
	number    = {2},
	pages     = {91--110},
	doi       = {10.1023/B:VISI.0000029664.99615.94},
	url		  = {https://doi.org/10.1023/B:VISI.0000029664.99615.94}
}

@ARTICLE{unet,
	author={Siddique, Nahian and Paheding, Sidike and Elkin, Colin P. and Devabhaktuni, Vijay},
	journal={IEEE Access}, 
	title={U-Net and Its Variants for Medical Image Segmentation: A Review of Theory and Applications}, 
	year={2021},
	volume={9},
	number={},
	pages={82031-82057},
	keywords={Image segmentation;Convolution;Biomedical imaging;Three-dimensional displays;Logic gates;Deep learning;Computer architecture;Biomedical imaging;deep learning;neural network architecture;segmentation;U-net},
	doi={10.1109/ACCESS.2021.3086020},
	url={https://doi.org/10.1109/ACCESS.2021.3086020}
}

@ARTICLE{spectralclustering,
	author={Jianbo Shi and Malik, J.},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Normalized cuts and image segmentation}, 
	year={2000},
	volume={22},
	number={8},
	pages={888-905},
	keywords={Image segmentation;Brightness;Clustering algorithms;Data mining;Eigenvalues and eigenfunctions;Bayesian methods;Coherence;Tree data structures;Filling;Partitioning algorithms},
	doi={10.1109/34.868688},
	url={https://doi.org/10.1109/34.868688}
}

@article{med1,
	author = {Santosh, Kc and Wendling, Laurent and Antani, Sameer and Thoma, George},
	year = {2015},
	month = {11},
	pages = {},
	title = {Overlaid arrow detection for labeling biomedical image regions},
	volume = {31},
	journal = {Intelligent Systems, IEEE},
	doi = {10.1109/MIS.2016.24},
	url = {https://doi.org/10.1109/MIS.2016.24}
}

@article{med2,
	author = {Santosh, Kc and Alam, Naved and Roy, Partha and Wendling, Laurent and Antani, Sameer and Thoma, George},
	year = {2016},
	month = {04},
	pages = {},
	title = {A Simple and Efficient Arrowhead Detection Technique in Biomedical Images},
	volume = {30},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	doi = {10.1142/S0218001416570020},
	url = {https://doi.org/10.1142/S0218001416570020}
}

@inproceedings{online1,
	author = {Bresler, Martin and Phan, Truyen and Průša, Daniel and Nakagawa, Masaki and Hlavac, Vaclav},
	year = {2014},
	month = {09},
	pages = {},
	title = {Recognition System for On-Line Sketched Diagrams},
	volume = {2014},
	journal = {Proceedings of International Conference on Frontiers in Handwriting Recognition, ICFHR},
	doi = {10.1109/ICFHR.2014.100},
	url = {https://doi.org/10.1109/ICFHR.2014.100}
}

@article{online2,
	author = {Bresler, Martin and Průša, Daniel and Hlavac, Vaclav},
	year = {2015},
	month = {02},
	pages = {610-617},
	title = {Detection of Arrows in On-Line Sketched Diagrams Using Relative Stroke Positioning},
	journal = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
	doi = {10.1109/WACV.2015.87},
	url = {https://doi.org/10.1109/WACV.2015.87}
}

@article{online3,
	author = {Bresler, Martin and Průša, Daniel and Hlavac, Vaclav},
	year = {2016},
	month = {09},
	pages = {},
	title = {Online recognition of sketched arrow-connected diagrams},
	volume = {19},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	doi = {10.1007/s10032-016-0269-z},
	url = {https://doi.org/10.1007/s10032-016-0269-z}
}

@online{ohfcd,
	title        = {OHFCD - Online Handwritten Flowchart Dataset},
	author       = {Schäfer, Bernhard and Keuper, Margret and Stuckenschmidt, Heiner},
	year         = {2021},
	howpublished = {\url{https://tc11.cvc.uab.es/datasets/OHFCD_1}},
	note         = {Accessed: 29/06/2025},
	institution  = {TC11 - IAPR Technical Committee on Reading Systems}
}

@inproceedings{BPMN,
	title = {Sketch2BPMN: Automatic Recognition of Hand-drawn BPMN Models},
	author = {Sch{\"a}fer, Bernhard and {van der Aa}, Han and Leopold, Henrik and Stuckenschmidt, Heiner},
	booktitle = {Advanced Information Systems Engineering},
	year = {2021},
	month = {06},
	pages = {344-360},
	publisher = {{Springer International Publishing}},
	language = {en},
	isbn = {978-3-030-79381-4},
	series = {Lecture {{Notes}} in {{Computer Science}}},
	doi = {10.1007/978-3-030-79382-1_21}
	url = {https://doi.org/10.1007/978-3-030-79382-1_21}
}

@online{modelsketch,
	author       = {Piucco, Letícia and Dos Santos, Edson Luiz and Johann, Marcelo and Oliveira, Lucas M.},
	title        = {ModelSketch: A Tool for Extracting UML Class Diagrams from Hand-Drawn Sketches},
	year         = {2021},
	howpublished = {\url{https://github.com/leticiapiucco/ModelSketch}},
	note         = {Accessed: 2025-06-29}
}

@online{circuitnet,
	author       = {Anthony, Anthony and Lee, Nathan and Sabri, Sarah and Wu, Zhiyuan},
	title        = {CircuitNet: A Dataset for Hand-drawn Circuit Diagram Recognition},
	year         = {2021},
	howpublished = {\url{https://github.com/aaanthonyyy/CircuitNet}},
	note         = {Accessed: 2025-06-29}
}

@InProceedings{ai2d,
	author="Kembhavi, Aniruddha
	and Salvato, Mike
	and Kolve, Eric
	and Seo, Minjoon
	and Hajishirzi, Hannaneh
	and Farhadi, Ali",
	editor="Leibe, Bastian
	and Matas, Jiri
	and Sebe, Nicu
	and Welling, Max",
	title="A Diagram is Worth a Dozen Images",
	booktitle="Computer Vision -- ECCV 2016",
	year="2016",
	publisher="Springer International Publishing",
	address="Cham",
	pages="235--251",
	abstract="Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for about 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.",
	isbn="978-3-319-46493-0"
}

@INPROCEEDINGS {lin2017feature,
	author = { Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge },
	booktitle = { 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) },
	title = {{ Feature Pyramid Networks for Object Detection }},
	year = {2017},
	volume = {},
	ISSN = {1063-6919},
	pages = {936-944},
	abstract = { Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available. },
	keywords = {Feature extraction;Detectors;Semantics;Computer architecture;Proposals;Object detection;Robustness},
	doi = {10.1109/CVPR.2017.106},
	url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.106},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month =Jul
}

@InProceedings{coco,
	author="Lin, Tsung-Yi
	and Maire, Michael
	and Belongie, Serge
	and Hays, James
	and Perona, Pietro
	and Ramanan, Deva
	and Doll{\'a}r, Piotr
	and Zitnick, C. Lawrence",
	editor="Fleet, David
	and Pajdla, Tomas
	and Schiele, Bernt
	and Tuytelaars, Tinne",
	title="Microsoft COCO: Common Objects in Context",
	booktitle="Computer Vision -- ECCV 2014",
	year="2014",
	publisher="Springer International Publishing",
	address="Cham",
	pages="740--755",
	abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
	isbn="978-3-319-10602-1"
}